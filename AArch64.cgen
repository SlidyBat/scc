arch AArch64 64

#define COND_EQ 0
#define COND_NE 1
#define COND_CS 2
#define COND_CC 3
#define COND_MI 4
#define COND_PL 5
#define COND_VS 6
#define COND_VC 7
#define COND_HI 8
#define COND_LS 9
#define COND_GE 10
#define COND_LT 11
#define COND_GT 12
#define COND_LE 13
#define COND_AL 14
#define UNCONDITIONAL 15

#define REG_FP 29
#define REG_LR 30

#define REG_SP 31

registerclass default IREG(8,16,32,64)
registerclass INTEGER_RETURN_VALUE(8,16,32,64) fixed { 0 } : IREG
registerclass SYSCALL_RETURN(8,16,32,64) fixed { 0 } : IREG
tempregisterclass IRESULT(8,16,32,64) IREG

registerclass INTEGER_PARAM_0(8,16,32,64) fixed { 0 } : IREG
registerclass INTEGER_PARAM_1(8,16,32,64) fixed { 1 } : IREG
registerclass INTEGER_PARAM_2(8,16,32,64) fixed { 2 } : IREG
registerclass INTEGER_PARAM_3(8,16,32,64) fixed { 3 } : IREG
registerclass INTEGER_PARAM_4(8,16,32,64) fixed { 4 } : IREG
registerclass INTEGER_PARAM_5(8,16,32,64) fixed { 5 } : IREG
registerclass INTEGER_PARAM_6(8,16,32,64) fixed { 6 } : IREG
registerclass INTEGER_PARAM_7(8,16,32,64) fixed { 7 } : IREG

registerclass SYSCALL_PARAM_0(8,16,32,64) fixed { 0 } : IREG
registerclass SYSCALL_PARAM_1(8,16,32,64) fixed { 1 } : IREG
registerclass SYSCALL_PARAM_2(8,16,32,64) fixed { 2 } : IREG
registerclass SYSCALL_PARAM_3(8,16,32,64) fixed { 3 } : IREG
registerclass SYSCALL_PARAM_4(8,16,32,64) fixed { 4 } : IREG
registerclass SYSCALL_PARAM_5(8,16,32,64) fixed { 5 } : IREG
registerclass SYSCALL_PARAM_6(8,16,32,64) fixed { 6 } : IREG
registerclass SYSCALL_PARAM_7(8,16,32,64) fixed { 7 } : IREG
registerclass SYSCALL_NUM(64) fixed { 8 } : IREG


function bool logical_imm_mask_test(uint64_t imm)
{
	imm = (imm - 1) | imm;
	return !((imm + 1) & imm);
}

// FIXME TODO 64-bit only right now
immediateclass LOGICALIMM
{
	uint64_t imm = value;
	uint64_t regMask = ((uint64_t) -1);
	if (imm & ~regMask) return false;
	if ((imm == 0) || (imm == regMask)) return false;

	uint32_t elementSize = 64;
	uint64_t elementMask;
	while (elementSize > 2)
	{
		elementSize /= 2;
		elementMask = ((uint64_t) -1) >> (64 - elementSize);
		if (((imm >> elementSize) & elementMask) != (imm & elementMask))
		{
			elementSize *= 2;
			elementMask = ((uint64_t) -1) >> (64 - elementSize);
			break;
		}
	}

	uint64_t elementValue = imm & elementMask;
	if (logical_imm_mask_test(elementValue))
		return true;
	if (logical_imm_mask_test(~(elementValue | ~elementMask)))
		return true;
	return false;
}

immediateclass IMM16  { return (value >= 0) && (value < 0x10000); }
immediateclass IMM16NOT  { return (value >= -0x10000) && (value < 0); }

immediateclass IMM9SIGNED { return (value > -0x100) && (value < 0x100); }

immediateclass IMM8  { return (value >= 0) && (value < 0x100); }
immediateclass IMM8NEG  { return (value > -0x100) && (value < 0); }
immediateclass IMM12  { return (value >= 0) && (value < 0x1000); }

callersaved {  0,  1,  2,  3,  4,  5,  6,  7,      9, 10, 11, 12, 13, 14, 15 }
calleesaved {             19, 20, 21, 22, 23, 24, 25, 26, 27, 28             }

special SYMREG_SP { SYMREG_NATIVE_REG(REG_SP) }
special SYMREG_BP { SYMREG_NATIVE_REG(REG_FP) }
special SYMREG_LR { SYMREG_NATIVE_REG(REG_LR) }

encoding DATA_IMM_PC { op=0:1, immlo:2, 0x4:3, op0=0:2, immhi:19, rd:5 }
encoding DATA_IMM_ADDSUB { sf:1, op:1, s:1, 0x11:5, shift=0:2, imm12:12, rn:5, rd:5 }
encoding DATA_IMM_LOGICAL { sf:1, opc:2, 0x24:6, n:1, immr:6, imms:6, rn:5, rd:5 }
encoding DATA_IMM_MOVW { sf:1, opc:2, 0x25:6, hw:2, imm16:16, rd:5 }
encoding DATA_IMM_BITFIELD { sf:1, opc:2, 0x26:6, n:1, immr:6, imms:6, rn:5, rd:5 }
encoding DATA_IMM_EXTRACT { sf:1, op21:2, 0x27:6, n:1, o0:1, rm:5, imms:6, rn:5, rd:5 }

encoding COND_BRANCH_IMM { 0x2a:7, o1=0:1, imm19:19, o0=0:1, cond=COND_AL:4 }
encoding EXCEPTION { 0xd4:8, opc:3, imm16:16, op2:3, ll:2 }
encoding SYSTEM { 0x354:10, l:1, op0:2, op1:3, crn:4, crm:4, op2:3, rt:5 }
encoding BRANCH_REG { 0x6b:7, opc:4, op2:5, op3:6, rn:5, op4:5 }
encoding BRANCH_IMM { op:1, 0x5:5, imm26:26 }
encoding COMP_BRANCH { sf:1, 0x1a:6, op:1, imm19:19, rt:5 }
encoding TEST_BRANCH { b5:1, 0x1b:6, op:1, b40:5, imm14:14, rt:5 }

encoding LOAD_REG_LIT { opc:2, 3:3, v=0:1, 0:2, imm19:19, rt:5 }
encoding LOAD_STORE_PAIR_POST { opc:2, 5:3, v=0:1, 1:3, l:1, imm7:7, rt2:5, rn:5, rt:5 }
encoding LOAD_STORE_PAIR      { opc:2, 5:3, v=0:1, 2:3, l:1, imm7:7, rt2:5, rn:5, rt:5 }
encoding LOAD_STORE_PAIR_PRE  { opc:2, 5:3, v=0:1, 3:3, l:1, imm7:7, rt2:5, rn:5, rt:5 }

encoding LOAD_STORE_IMM      { size:2, 7:3, v=0:1, 0:2, opc:2, 0:1, imm9:9, 0:2, rn:5, rt:5 }
encoding LOAD_STORE_IMM_POST { size:2, 7:3, v=0:1, 0:2, opc:2, 0:1, imm9:9, 1:2, rn:5, rt:5 }
encoding LOAD_STORE_IMM_PRE  { size:2, 7:3, v=0:1, 0:2, opc:2, 0:1, imm9:9, 3:2, rn:5, rt:5 }

encoding LOAD_STORE_REG_OFF { size:2, 7:3, v=0:1, 0:2, opc:2, 1:1, rm:5, option:3, s:1, 2:2, rn:5, rt:5 }

encoding LOAD_STORE_UIMM { size:2, 7:3, v=0:1, 1:2, opc:2, imm12:12, rn:5, rt:5 }

encoding DATA_REG_2SRC { sf:1, 0:1, s:1, 0xd6:8, rm:5, opcode:6, rn:5, rd:5 }
encoding DATA_REG_1SRC { sf:1, 1:1, s:1, 0xd6:8, opcode2:5, opcode:6, rn:5, rd:5 }
encoding DATA_REG_LOGICAL_SHIFTED { sf:1, opc:2, 0xa:5, shift:2, n:1, rm:5, imm6:6, rn:5, rd:5 }
encoding DATA_REG_ADDSUB_SHIFTED  { sf:1, op:1, s:1, 0xb:5, shift:2, 0:1, rm:5, imm6:6, rn:5, rd:5 }
encoding DATA_REG_ADDSUB_EXTENDED { sf:1, op:1, s:1, 0xb:5, opt:2, 1:1, rm:5, option:3, imm3:3, rn:5, rd:5 }

encoding COND_CMP { sf:1, op:1, s=1:1, 0xd2:8, rm:5, cond=COND_AL:4, 0:1, o2=0:1, rn:5, o3=0:1, nzcv:4 }
encoding COND_SEL { sf:1, op:1, s=0:1, 0xd4:8, rm:5, cond=COND_AL:4, op2:2, rn:5, rd:5 }

encoding DATA_REG_3SRC { sf:1, op54:2, 0x1b:5, op31:3, rm:5, o0:1, ra:5, rn:5, rd:5 }



instr function int64_t RelativeAddRead21(OutputBlock* out, Relocation& reloc)
{
	uint32_t instr = out->ReadOffsetUInt32(reloc.instruction);
	int32_t diff = ((instr & ~0x1f) << 8);
	diff >>= 11;
	diff |= (instr >> 31) & 3;
	return diff;
}

instr function void RelativeAddWrite21(OutputBlock* out, Relocation& reloc, int64_t diff)
{
	uint32_t instr = out->ReadOffsetUInt32(reloc.instruction);
	instr &= 0x9f00001f;
	instr |= (diff & 3) << 29;
	instr |= (diff & 0x1ffffc) << 3;
	out->WriteOffsetUInt32(reloc.instruction, instr);
}

instr function bool RelativeAddValid21(OutputBlock* out, Relocation& reloc, int64_t diff)
{
	return (diff > -0x100000) && (diff < 0x100000);
}

instr add_global  adr rd:REG(w), var:GLOBALVAR
{
	@DATA_IMM_PC immlo=0, immhi=0;

	Relocation reloc;
	reloc.type = DATA_RELOC_RELATIVE_CUSTOM_FIELD;
	reloc.instruction = out->len - 4;
	reloc.offset = out->len - 4;
	reloc.dataOffset = %var:offset;
	reloc.bitSize = 21;
	reloc.overflow = NULL;
	reloc.read = RelativeAddRead21;
	reloc.write = RelativeAddWrite21;
	reloc.valid = RelativeAddValid21;
	out->relocs.push_back(reloc);
}

instr add_block  adr rd:REG(w), func:FUNCTION
{
	@DATA_IMM_PC immlo=0, immhi=0;

	Relocation reloc;
	reloc.type = CODE_RELOC_RELATIVE_CUSTOM_FIELD;
	reloc.instruction = out->len - 4;
	reloc.offset = out->len - 4;
	reloc.target = %func:block;
	reloc.bitSize = 21;
	reloc.overflow = NULL;
	reloc.read = RelativeAddRead21;
	reloc.write = RelativeAddWrite21;
	reloc.valid = RelativeAddValid21;
	out->relocs.push_back(reloc);
}

instr add_stack  add rd:REG(w), var:STACKVAR
{
	if ((rd == SYMREG_NATIVE_REG(REG_SP)) && (rd == %var:base) && (%var:offset == 0))
		return true;
	if ((%var:offset > -0x1000) && (%var:offset < 0))
		@DATA_IMM_ADDSUB sf=1, op=1, s=0, imm12=-%var:offset, rn=%var:base; // sub
	else if ((%var:offset < 0x1000) && (%var:offset >= 0))
		@DATA_IMM_ADDSUB sf=1, op=0, s=0, imm12=%var:offset, rn=%var:base; // add
	else if ((%var:offset > -0x1000000) && (%var:offset < 0))
	{
		uint32_t offsetHigh = (-%var:offset) >> 12;
		uint32_t offsetLow  = (-%var:offset) & 0xfff;

		if (offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=1, s=0, shift=1, imm12=offsetHigh, rn=%var:base, rd=(offsetLow ? %var:temp : rd); // sub
		if (offsetLow || !offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=1, s=0, imm12=offsetLow, rn=(offsetHigh ? %var:temp : %var:base); // sub
	}
	else if ((%var:offset < 0x1000000) && (%var:offset >= 0))
	{
		uint32_t offsetHigh = (%var:offset) >> 12;
		uint32_t offsetLow  = (%var:offset) & 0xfff;

		if (offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=0, s=0, shift=1, imm12=(%var:offset)>>12, rn=%var:base, rd=(offsetLow ? %var:temp : rd); // add
		if (offsetLow || !offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=0, s=0, imm12=offsetLow, rn=(offsetHigh ? %var:temp : %var:base); // add
	}
	else
	{
		return false;
	}
}

instr sub_stack  sub rd:REG(w), var:STACKVAR
{
	if ((rd == SYMREG_NATIVE_REG(REG_SP)) && (rd == %var:base) && (%var:offset == 0))
		return true;
	if ((%var:offset > -0x1000) && (%var:offset < 0))
		@DATA_IMM_ADDSUB sf=1, op=0, s=0, imm12=-%var:offset, rn=%var:base; // add
	else if ((%var:offset < 0x1000) && (%var:offset >= 0))
		@DATA_IMM_ADDSUB sf=1, op=1, s=0, imm12=%var:offset, rn=%var:base; // sub
	else if ((%var:offset > -0x1000000) && (%var:offset < 0))
	{
		uint32_t offsetHigh = (-%var:offset) >> 12;
		uint32_t offsetLow  = (-%var:offset) & 0xfff;

		if (offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=0, s=0, shift=1, imm12=offsetHigh, rn=%var:base, rd=(offsetLow ? %var:temp : rd); // add
		if (offsetLow || !offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=0, s=0, imm12=offsetLow, rn=(offsetHigh ? %var:temp : %var:base); // add
	}
	else if ((%var:offset < 0x1000000) && (%var:offset >= 0))
	{
		uint32_t offsetHigh = (%var:offset) >> 12;
		uint32_t offsetLow  = (%var:offset) & 0xfff;

		if (offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=1, s=0, shift=1, imm12=offsetHigh, rn=%var:base, rd=(offsetLow ? %var:temp : rd); // sub
		if (offsetLow || !offsetHigh)
			@DATA_IMM_ADDSUB sf=1, op=1, s=0, imm12=offsetLow, rn=(offsetHigh ? %var:temp : %var:base); // sub
	}
	else
	{
		return false;
	}
}


instr function bool logical_imm_mask_test(uint64_t imm)
{
	imm = (imm - 1) | imm;
	return !((imm + 1) & imm);
}


instr function bool logical_imm_encode(uint64_t imm, uint32_t regSize,
		uint32_t* n, uint32_t* immr, uint32_t* imms)
{
	uint64_t regMask = ((uint64_t) -1) >> (64 - regSize);
	if (imm & ~regMask) return false;
	if ((imm == 0) || (imm == regMask)) return false;

	uint32_t elementSize = regSize;
	uint64_t elementMask;
	while (elementSize > 2)
	{
		elementSize /= 2;
		elementMask = ((uint64_t) -1) >> (64 - elementSize);
		if (((imm >> elementSize) & elementMask) != (imm & elementMask))
		{
			elementSize *= 2;
			elementMask = ((uint64_t) -1) >> (64 - elementSize);
			break;
		}
	}

	uint64_t elementValue = imm & elementMask;
	uint32_t rot, run;
	if (logical_imm_mask_test(elementValue))
	{
		rot = __builtin_ctzl(elementValue);
		run = __builtin_ctzl(~(elementValue >> rot));
	}
	else
	{
		elementValue |= ~elementMask;
		if (!logical_imm_mask_test(~elementValue))
			return false;
		rot = 64 - __builtin_clzl(~elementValue);
		run = (64 - rot) + __builtin_ctzl(~elementValue) - (64 - elementSize);
	}

	if (n && immr && imms)
	{
		*immr = (elementSize - rot) & (elementSize - 1);
		*imms = (~(elementSize - 1) << 1) | (run - 1);
		*n = ((*imms >> 6) & 1) ^ 1;
		*imms &= 0x3f;
	}

	return true;
}

#define DATA_LOGICAL(name, inverse, opcode, is64) \
instr name            name rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=0, shift=0; } \
instr name##_lsl_imm  name rd:REG(w), rn:REG(r), rm:REG(r) lsl imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=0, shift=0; } \
instr name##_lsr_imm  name rd:REG(w), rn:REG(r), rm:REG(r) lsr imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=0, shift=1; } \
instr name##_asr_imm  name rd:REG(w), rn:REG(r), rm:REG(r) asr imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=0, shift=2; } \
instr inverse            inverse rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=1, shift=0; } \
instr inverse##_lsl_imm  inverse rd:REG(w), rn:REG(r), rm:REG(r) lsl imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=1, shift=0; } \
instr inverse##_lsr_imm  inverse rd:REG(w), rn:REG(r), rm:REG(r) lsr imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=1, shift=1; } \
instr inverse##_asr_imm  inverse rd:REG(w), rn:REG(r), rm:REG(r) asr imm6:IMM  { @DATA_REG_LOGICAL_SHIFTED sf=is64, opc=opcode, n=1, shift=2; } \
instr name##_imm      name rd:REG(w), rn:REG(r), i:IMM  { \
	uint32_t n, immr, imms; \
	if (!logical_imm_encode(i, (is64) ? 64 : 32, &n, &immr, &imms)) \
		return false; \
	@DATA_IMM_LOGICAL sf=is64, opc=opcode, n=n, immr=immr, imms=imms; \
}
DATA_LOGICAL(and, bic, 0, 1)
DATA_LOGICAL(orr, orn, 1, 1)
DATA_LOGICAL(eor, eon, 2, 1)
DATA_LOGICAL(andw, bicw, 0, 0)
DATA_LOGICAL(orrw, ornw, 1, 0)
DATA_LOGICAL(eorw, eonw, 2, 0)

instr mvnw  mvnw rd:REG(w), rm:REG(r)  { @DATA_REG_LOGICAL_SHIFTED sf=0, opc=1, n=1, shift=0, rn=0x1f; }
instr mvnx  mvnx rd:REG(w), rm:REG(r)  { @DATA_REG_LOGICAL_SHIFTED sf=1, opc=1, n=1, shift=0, rn=0x1f; }

#define DATA_ADDSUB(name, opcode) \
instr name             name  rd:REG(w), rn:REG(r), rm:REG(r)               { @DATA_REG_ADDSUB_SHIFTED sf=1, op=opcode, s=0, shift=0, imm6=0; } \
instr name##_lsl_imm   name  rd:REG(w), rn:REG(r), rm:REG(r) lsl imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=1, op=opcode, s=0, shift=0; } \
instr name##_lsr_imm   name  rd:REG(w), rn:REG(r), rm:REG(r) lsr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=1, op=opcode, s=0, shift=1; } \
instr name##_asr_imm   name  rd:REG(w), rn:REG(r), rm:REG(r) asr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=1, op=opcode, s=0, shift=2; } \
instr name##_imm       name  rd:REG(w), rn:REG(r), imm12:IMM { @DATA_IMM_ADDSUB sf=1, op=opcode, s=0; } \
instr name##w          namew rd:REG(w), rn:REG(r), rm:REG(r)               { @DATA_REG_ADDSUB_SHIFTED sf=0, op=opcode, s=0, shift=0, imm6=0; } \
instr name##w_lsl_imm  namew rd:REG(w), rn:REG(r), rm:REG(r) lsl imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=0, op=opcode, s=0, shift=0; } \
instr name##w_lsr_imm  namew rd:REG(w), rn:REG(r), rm:REG(r) lsr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=0, op=opcode, s=0, shift=1; } \
instr name##w_asr_imm  namew rd:REG(w), rn:REG(r), rm:REG(r) asr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=0, op=opcode, s=0, shift=2; } \
instr name##w_imm      namew rd:REG(w), rn:REG(r), imm12:IMM { @DATA_IMM_ADDSUB sf=0, op=opcode, s=0; }
DATA_ADDSUB(add, 0)
DATA_ADDSUB(sub, 1)

instr negw  negw rd:REG(w), rm:REG(r) { @DATA_REG_ADDSUB_SHIFTED sf=0, op=1, s=0, s=0, imm6=0, rn=0x1f; }
instr negx  negx rd:REG(w), rm:REG(r) { @DATA_REG_ADDSUB_SHIFTED sf=1, op=1, s=0, s=0, imm6=0, rn=0x1f; }

#define COMPARE_INSTR(name, opcode, is64) \
instr(writeflags) name            name  rn:REG(r), rm:REG(r)               { @DATA_REG_ADDSUB_SHIFTED sf=is64, op=opcode, s=1, shift=0, imm6=0, rd=0x1f; } \
instr(writeflags) name##_lsl_imm  name  rn:REG(r), rm:REG(r) lsl imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=is64, op=opcode, s=1, shift=0, rd=0x1f; } \
instr(writeflags) name##_lsr_imm  name  rn:REG(r), rm:REG(r) lsr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=is64, op=opcode, s=1, shift=1, rd=0x1f; } \
instr(writeflags) name##_asr_imm  name  rn:REG(r), rm:REG(r) asr imm6:IMM  { @DATA_REG_ADDSUB_SHIFTED sf=is64, op=opcode, s=1, shift=2, rd=0x1f; } \
instr(writeflags) name##_imm      name  rn:REG(r), imm12:IMM { @DATA_IMM_ADDSUB sf=is64, op=opcode, s=1, rd=0x1f; }
COMPARE_INSTR(cmnx, 0x0, 1)
COMPARE_INSTR(cmpx, 0x1, 1)
COMPARE_INSTR(cmnw, 0x0, 0)
COMPARE_INSTR(cmpw, 0x1, 0)

instr(copy) mov  mov rd:REG(w), rm:REG(r) {
	if (((rm&0x1f) == 31) || ((rd&0x1f) == 31)) {
		@DATA_IMM_ADDSUB sf=1, op=0, s=0, imm12=0, rn=rm;
	} else {
		@DATA_REG_LOGICAL_SHIFTED sf=1, opc=1, n=0, shift=0, rn=0x1f, imm6=0;
	}
} update {
	if (%rd == %rm)
		return true;
	return false;
}

instr(copy) movw  movw rd:REG(w), rm:REG(r) {
	@DATA_REG_LOGICAL_SHIFTED sf=0, opc=1, n=0, shift=0, rn=0x1f, imm6=0;
} update {
	if (%rd == %rm)
		return true;
	return false;
}

instr movn  mov rd:REG(w), imm16:IMM  { @DATA_IMM_MOVW sf=1, opc=0, hw=0; }
instr movz  mov rd:REG(w), imm16:IMM  { @DATA_IMM_MOVW sf=1, opc=2, hw=0; }
instr movk  mov rd:REG(rw), imm16:IMM, shift:IMM  { @DATA_IMM_MOVW sf=1, opc=3, hw=shift; }

instr sxtb  sxtb rd:REG(w), rn:REG(r) { @DATA_IMM_BITFIELD sf=1, opc=0, n=1, immr=0, imms=0x07; }
instr sxth  sxth rd:REG(w), rn:REG(r) { @DATA_IMM_BITFIELD sf=1, opc=0, n=1, immr=0, imms=0x0f; }
instr sxtw  sxtw rd:REG(w), rn:REG(r) { @DATA_IMM_BITFIELD sf=1, opc=0, n=1, immr=0, imms=0x1f; }

instr uxtb  uxtb rd:REG(w), rn:REG(r) { @DATA_IMM_BITFIELD sf=0, opc=2, n=0, immr=0, imms=0x7; }
instr uxth  uxth rd:REG(w), rn:REG(r) { @DATA_IMM_BITFIELD sf=0, opc=2, n=0, immr=0, imms=0xf; }

#define COND_BRANCH_RELOC(dest) \
	Relocation reloc; \
	reloc.type = CODE_RELOC_RELATIVE_32_FIELD; \
	reloc.overflow = NULL; \
	reloc.instruction = out->len - 4; \
	reloc.offset = out->len - 4; \
	reloc.target = dest; \
	reloc.bitOffset = 5; \
	reloc.bitSize = 19; \
	reloc.bitShift = 2; \
	out->relocs.push_back(reloc);

instr(branch) beq  beq dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_EQ, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bne  bne dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_NE, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bcs  bcs dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_CS, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bcc  bcc dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_CC, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bmi  bmi dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_MI, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bpl  bpl dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_PL, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bvs  bvs dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_VS, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bvc  bvc dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_VC, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bhi  bhi dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_HI, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bls  bls dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_LS, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bge  bge dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_GE, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) blt  blt dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_LT, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) bgt  bgt dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_GT, imm19=1; COND_BRANCH_RELOC(%dest:block); }
instr(branch) ble  ble dest:FUNCTION  { @COND_BRANCH_IMM cond=COND_LE, imm19=1; COND_BRANCH_RELOC(%dest:block); }

#define BRANCH_RELOC(dest) \
	Relocation reloc; \
	reloc.type = CODE_RELOC_RELATIVE_32_FIELD; \
	reloc.overflow = NULL; \
	reloc.instruction = out->len - 4; \
	reloc.offset = out->len - 4; \
	reloc.target = dest; \
	reloc.bitOffset = 0; \
	reloc.bitSize = 26; \
	reloc.bitShift = 2; \
	out->relocs.push_back(reloc);

instr(branch) b  b dest:FUNCTION
{
	@BRANCH_IMM op=0, imm26=1;
	BRANCH_RELOC(%dest:block);
}

instr(call) bl  bl dest:FUNCTION, retval:REG(w), reads:REGLIST(r)
{
	@BRANCH_IMM op=1, imm26=1;
	BRANCH_RELOC(%dest:block);
}

instr(branch)  br   br rn:REG(r)  { @BRANCH_REG opc=0, op2=0x1f, op3=0, op4=0; }
instr(branch) ret  ret rn:REG(r)  { @BRANCH_REG opc=2, op2=0x1f, op3=0, op4=0; }
instr(call)   blr  blr rn:REG(r), retval:REG(w), reads:REGLIST(r)
{
	@BRANCH_REG opc=1, op2=0x1f, op3=0, op4=0;
}


instr(call) svc  svc num:REG(r), writes:REGLIST(w), reads:REGLIST(r)  { @EXCEPTION opc=0, op2=0, imm16=0, ll=1; }
instr brk  brk  { @EXCEPTION opc=1, op2=0, ll=0, imm16=0; }

instr rev16  rev16 rd:REG(w), rn:REG(r)  { @DATA_REG_1SRC sf=1, s=0, opcode2=0, opcode=1; }
instr rev32  rev32 rd:REG(w), rn:REG(r)  { @DATA_REG_1SRC sf=1, s=0, opcode2=0, opcode=2; }
instr rev64  rev64 rd:REG(w), rn:REG(r)  { @DATA_REG_1SRC sf=1, s=0, opcode2=0, opcode=3; }


instr lsl_imm  lsl rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=1, opc=2, n=1, immr=(-%i % 64), imms=63-i; }
instr lsr_imm  lsr rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=1, opc=2, n=1, immr=i, imms=63; }
instr asr_imm  asr rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=1, opc=0, n=1, immr=i, imms=63; }

instr lslw_imm  lslw rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=0, opc=2, n=0, immr=(-%i % 32), imms=31-i; }
instr lsrw_imm  lsrw rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=0, opc=2, n=0, immr=i, imms=31; }
instr asrw_imm  asrw rd:REG(w), rn:REG(r), i:IMM  { @DATA_IMM_BITFIELD sf=0, opc=0, n=0, immr=i, imms=31; }


instr lsl  lsl rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=1, s=0, opcode=0x08; }
instr lsr  lsr rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=1, s=0, opcode=0x09; }
instr asr  asr rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=1, s=0, opcode=0x0a; }

instr lslw  lslw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=0, s=0, opcode=0x08; }
instr lsrw  lsrw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=0, s=0, opcode=0x09; }
instr asrw  asrw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=0, s=0, opcode=0x0a; }


instr   mul     mul rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_3SRC sf=1, op54=0, op31=0, o0=0, ra=0x1f; }
instr  mulw    mulw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_3SRC sf=0, op54=0, op31=0, o0=0, ra=0x1f; }
instr smull   smull rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_3SRC sf=1, op54=0, op31=1, o0=0, ra=0x1f; }
instr umull   umull rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_3SRC sf=1, op54=0, op31=5, o0=0, ra=0x1f; }
instr  msub    msub rd:REG(w), rn:REG(r), rm:REG(r) ra:REG(r) { @DATA_REG_3SRC sf=1, op54=0, op31=0, o0=1; }
instr msubw   msubw rd:REG(w), rn:REG(r), rm:REG(r) ra:REG(r) { @DATA_REG_3SRC sf=0, op54=0, op31=0, o0=1; }

//encoding DATA_REG_2SRC { sf:1, 0:1, s:1, 0xd6:8, rm:5, opcode:6, rn:5, rd:5 }
instr udiv  udiv rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=1, s=0, opcode=0x02; }
instr sdiv  sdiv rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=1, s=0, opcode=0x03; }

instr udivw  udivw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=0, s=0, opcode=0x02; }
instr sdivw  sdivw rd:REG(w), rn:REG(r), rm:REG(r)  { @DATA_REG_2SRC sf=0, s=0, opcode=0x03; }

#define MEMORY_INSTR(name, opsize, opcode, effect) \
instr(memory) name##_reg      name rt:REG(effect), [rn:REG(r) + rm:REG(r)] { @LOAD_STORE_REG_OFF size=opsize, opc=opcode, option=3, s=0; } \
instr(memory) name##_imm      name rt:REG(effect), [rn:REG(r) + imm:IMM]   { @LOAD_STORE_IMM     size=opsize, opc=opcode, imm9=imm; } \
instr(memory) name##_imm_pre  name rt:REG(effect), [rn:REG(rw) + imm:IMM]! { @LOAD_STORE_IMM_PRE size=opsize, opc=opcode, imm9=imm; } \
instr(memory) name##_uimm     name rt:REG(effect), [rn:REG(r) + imm:IMM]   { @LOAD_STORE_UIMM    size=opsize, opc=opcode, imm12=imm>>opsize; } \
instr(memory) name##_stack    name rt:REG(effect), [var:STACKVAR] \
{ \
	if ((%var:offset >= 0) && (%var:offset>>opsize < 0x1000) && !(%var:offset & ((1 << (opsize * 8))-1))) \
	{ \
		@LOAD_STORE_UIMM size=opsize, opc=opcode, rn=%var:base, imm12=%var:offset>>opsize; \
	} \
	else if ((%var:offset > -0x100) && (%var:offset < 0x100)) \
	{ \
		@LOAD_STORE_IMM size=opsize, opc=opcode, rn=%var:base, imm9=%var:offset; \
	} \
	else if ((%var:offset >= 0) && (%var:offset < 0x10000)) \
	{ \
		@DATA_IMM_MOVW sf=1, opc=2, hw=0, rd=%var:temp, imm16=%var:offset; \
		@LOAD_STORE_REG_OFF size=opsize, opc=opcode, option=3, s=0, rn=%var:base, rm=%var:temp; \
	} \
	else if ((%var:offset > -0x10000) && (%var:offset < 0)) \
	{ \
		@DATA_IMM_MOVW sf=1, opc=0, hw=0, rd=%var:temp, imm16=~%var:offset; \
		@LOAD_STORE_REG_OFF size=opsize, opc=opcode, option=3, s=0, rn=%var:base, rm=%var:temp; \
	} \
	else \
	{ \
		@DATA_IMM_MOVW sf=1, opc=2, hw=0, rd=%var:temp, imm16=%var:offset; \
		@DATA_IMM_MOVW sf=1, opc=3, hw=0, rd=%var:temp, imm16=%var:offset>>16; \
		@LOAD_STORE_REG_OFF size=opsize, opc=opcode, option=3, s=0, rn=%var:base, rm=%var:temp; \
	} \
}
MEMORY_INSTR( strb, 0, 0, r)
MEMORY_INSTR( ldrb, 0, 1, w)
MEMORY_INSTR(ldrsb, 0, 2, w)
MEMORY_INSTR( strh, 1, 0, r)
MEMORY_INSTR( ldrh, 1, 1, w)
MEMORY_INSTR(ldrsh, 1, 2, w)
MEMORY_INSTR( strw, 2, 0, r)
MEMORY_INSTR( ldrw, 2, 1, w)
MEMORY_INSTR(ldrsw, 2, 2, w)
MEMORY_INSTR( strx, 3, 0, r)
MEMORY_INSTR( ldrx, 3, 1, w)


#define MEMORY_PAIR_INSTR(name, opcode, load, effect) \
instr(memory) name         name rt:REG(effect), rt2:REG(effect), [rn:REG(r)  + imm7:IMM]   { @LOAD_STORE_PAIR      opc=opcode, l=load; } \
instr(memory) name##_pre   name rt:REG(effect), rt2:REG(effect), [rn:REG(rw) + imm7:IMM]!  { @LOAD_STORE_PAIR_PRE  opc=opcode, l=load; } \
instr(memory) name##_post  name rt:REG(effect), rt2:REG(effect), [rn:REG(rw)], imm7:IMM    { @LOAD_STORE_PAIR_POST opc=opcode, l=load; }
MEMORY_PAIR_INSTR(stp, 2, 0, r)
MEMORY_PAIR_INSTR(ldp, 2, 1, w)

instr saveregs  saveregs {} update
{
	vector<uint32_t> clobbered = func->GetClobberedCalleeSavedRegisters();
	uint32_t index = 0;
	uint32_t frameSize = 0x10 + (((clobbered.size() * 8) + 15) & ~0xf);
	@stp_pre REG_FP, REG_LR, SYMREG_SP, -(frameSize >> 3);
	for (vector<uint32_t>::iterator i = clobbered.begin(); i != clobbered.end(); index++)
	{
		uint32_t reg1 = *i++;
		uint32_t reg2 = (i != clobbered.end()) ? *i++ : 0x1f;
		@stp reg1, reg2, SYMREG_SP, ((index + 1) * 0x10) >> 3;
	}
	return true;
}

instr restoreregs  restoreregs {} update
{

	vector<uint32_t> clobbered = func->GetClobberedCalleeSavedRegisters();
	uint32_t index = 0;
	uint32_t frameSize = 0x10 + (((clobbered.size() * 8) + 15) & ~0xf);
	for (vector<uint32_t>::iterator i = clobbered.begin(); i != clobbered.end(); index++)
	{
		uint32_t reg1 = *i++;
		uint32_t reg2 = (i != clobbered.end()) ? *i++ : 0x1f;
		@ldp reg1, reg2, SYMREG_SP, ((index + 1) * 0x10) >> 3;
	}
	@ldp_post REG_FP, REG_LR, SYMREG_SP, (frameSize >> 3);
	return true;
}

// Data flow pseudo-instructions
instr regparam  regparam regs:REGLIST(w)  {} update { return true; }
instr symreturn  symreturn low:REG(r)  {} update { return true; }

src:IMM16 => dest:IREG { @movz %dest, %src; }
src:IMM16NOT => dest:IREG { @movn %dest, ~%src; }
src:IMM => dest:IREG
{
	uint16_t part;
	@movz %dest, %src;
	if ((part = (%src >> 16)))
		@movk %dest, part, 1;
	if ((part = (%src >> 32)))
		@movk %dest, part, 2;
	if ((part = (%src >> 48)))
		@movk %dest, part, 3;
}

src:IRESULT => dest:IREG(S8) { @sxtb %dest, %src; }
src:IRESULT => dest:IREG(U8) { @uxtb %dest, %src; }
src:IRESULT => dest:IREG(S16) { @sxth %dest, %src; }
src:IRESULT => dest:IREG(U16) { @uxth %dest, %src; }
src:IRESULT => dest:IREG(S32) { @sxtw %dest, %src; }
src:IRESULT => dest:IREG(U32) { @and_imm %dest, %src, 0xffffffff; }
src:IRESULT => dest:IREG(64) { }

func:FUNCTION => dest:IREG { @add_block %dest, %func, %func->GetIL()[0]; }

assign dest:IREG src:IREG { @mov %dest, %src; }

load(S8) base:IREG => dest:IREG { @ldrsb_uimm %dest, %base, 0; }
load(S8) add base:IREG ofs:IMM12 => dest:IREG { @ldrsb_uimm %dest, %base, %ofs; }
load(S8) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrsb_imm %dest, %base, %ofs; }
load(S8) add base:IREG ofs:IREG => dest:IREG { @ldrsb_reg %dest, %base, %ofs; }
load(U8) base:IREG => dest:IREG { @ldrb_uimm %dest, %base, 0; }
load(U8) add base:IREG ofs:IMM12 => dest:IREG { @ldrb_uimm %dest, %base, %ofs; }
load(U8) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrb_imm %dest, %base, %ofs; }
load(U8) add base:IREG ofs:IREG => dest:IREG { @ldrb_reg %dest, %base, %ofs; }
load(S16) base:IREG => dest:IREG { @ldrsh_uimm %dest, %base, 0; }
load(S16) add base:IREG ofs:IMM12 => dest:IREG { @ldrsh_uimm %dest, %base, %ofs; }
load(S16) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrsh_imm %dest, %base, %ofs; }
load(S16) add base:IREG ofs:IREG => dest:IREG { @ldrsh_reg %dest, %base, %ofs; }
load(U16) base:IREG => dest:IREG { @ldrh_uimm %dest, %base, 0; }
load(U16) add base:IREG ofs:IMM12 => dest:IREG { @ldrh_uimm %dest, %base, %ofs; }
load(U16) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrh_imm %dest, %base, %ofs; }
load(U16) add base:IREG ofs:IREG => dest:IREG { @ldrh_reg %dest, %base, %ofs; }
load(S32) base:IREG => dest:IREG { @ldrsw_uimm %dest, %base, 0; }
load(S32) add base:IREG ofs:IMM12 => dest:IREG { @ldrsw_uimm %dest, %base, %ofs; }
load(S32) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrsw_imm %dest, %base, %ofs; }
load(S32) add base:IREG ofs:IREG => dest:IREG { @ldrsw_reg %dest, %base, %ofs; }
load(U32) base:IREG => dest:IREG { @ldrw_uimm %dest, %base, 0; }
load(U32) add base:IREG ofs:IMM12 => dest:IREG { @ldrw_uimm %dest, %base, %ofs; }
load(U32) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrw_imm %dest, %base, %ofs; }
load(U32) add base:IREG ofs:IREG => dest:IREG { @ldrw_reg %dest, %base, %ofs; }
load(64) base:IREG => dest:IREG { @ldrx_uimm %dest, %base, 0; }
load(64) add base:IREG ofs:IMM12 => dest:IREG { @ldrx_uimm %dest, %base, %ofs; }
load(64) add base:IREG ofs:IMM9SIGNED => dest:IREG { @ldrx_imm %dest, %base, %ofs; }
load(64) add base:IREG ofs:IREG => dest:IREG { @ldrx_reg %dest, %base, %ofs; }

load ref src:STACKVAR(S8) => dest:IREG { @ldrsb_stack %dest, %src; }
load ref src:STACKVAR(U8) => dest:IREG { @ldrb_stack %dest, %src; }
load ref src:STACKVAR(S16) => dest:IREG { @ldrsh_stack %dest, %src; }
load ref src:STACKVAR(U16) => dest:IREG { @ldrh_stack %dest, %src; }
load ref src:STACKVAR(S32) => dest:IREG { @ldrsw_stack %dest, %src; }
load ref src:STACKVAR(U32) => dest:IREG { @ldrw_stack %dest, %src; }
load ref src:STACKVAR(64) => dest:IREG { @ldrx_stack %dest, %src; }

store(8) base:IREG src:IREG { @strb_uimm %src, %base, 0; }
store(8) add base:IREG ofs:IMM12 src:IREG { @strb_uimm %src, %base, %ofs; }
store(8) add base:IREG ofs:IMM9SIGNED src:IREG { @strb_imm %src, %base, %ofs; }
store(8) add base:IREG ofs:IREG src:IREG { @strb_reg %src, %base, %ofs; }
store(16) base:IREG src:IREG { @strh_uimm %src, %base, 0; }
store(16) add base:IREG ofs:IMM12 src:IREG { @strh_uimm %src, %base, %ofs; }
store(16) add base:IREG ofs:IMM9SIGNED src:IREG { @strh_imm %src, %base, %ofs; }
store(16) add base:IREG ofs:IREG src:IREG { @strh_reg %src, %base, %ofs; }
store(32) base:IREG src:IREG { @strw_uimm %src, %base, 0; }
store(32) add base:IREG ofs:IMM12 src:IREG { @strw_uimm %src, %base, %ofs; }
store(32) add base:IREG ofs:IMM9SIGNED src:IREG { @strw_imm %src, %base, %ofs; }
store(32) add base:IREG ofs:IREG src:IREG { @strw_reg %src, %base, %ofs; }
store(64) base:IREG src:IREG { @strx_uimm %src, %base, 0; }
store(64) add base:IREG ofs:IMM12 src:IREG { @strx_uimm %src, %base, %ofs; }
store(64) add base:IREG ofs:IMM9SIGNED src:IREG { @strx_imm %src, %base, %ofs; }
store(64) add base:IREG ofs:IREG src:IREG { @strx_reg %src, %base, %ofs; }

store ref dest:STACKVAR(8) src:IREG { @strb_stack %src, %dest; }
store ref dest:STACKVAR(16) src:IREG { @strh_stack %src, %dest; }
store ref dest:STACKVAR(32) src:IREG { @strw_stack %src, %dest; }
store ref dest:STACKVAR(64) src:IREG { @strx_stack %src, %dest; }

ref base:GLOBALVAR => dest:IREG { @add_global %dest, %base; }
ref base:STACKVAR => dest:IREG { @add_stack %dest, %base; }

add a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { @addw %dest, %a, %b; }
add a:IREG(8,16,32) b:IMM8 => dest:IRESULT { @addw_imm %dest, %a, %b; }
add a:IREG(8,16,32) b:IMM8NEG => dest:IRESULT { @sub_imm %dest, %a, %b; }
add a:IREG(8,16,32) shl b:IREG(8,16,32) c:IMM => dest:IRESULT { @addw_lsl_imm %dest, %a, %b, %c; }
add a:IREG(8,16,32) shr b:IREG(8,16,32) c:IMM => dest:IRESULT { @addw_lsr_imm %dest, %a, %b, %c; }
add a:IREG(8,16,32) sar b:IREG(8,16,32) c:IMM => dest:IRESULT { @addw_asr_imm %dest, %a, %b, %c; }

add a:IREG(64) b:IREG(64) => dest:IRESULT { @add %dest, %a, %b; }
add a:IREG(64) b:IMM8 => dest:IRESULT { @add_imm %dest, %a, %b; }
add a:IREG(64) b:IMM8NEG => dest:IRESULT { @sub_imm %dest, %a, %b; }
add a:IREG(64) shl b:IREG(64) c:IMM => dest:IRESULT { @add_lsl_imm %dest, %a, %b, %c; }
add a:IREG(64) shr b:IREG(64) c:IMM => dest:IRESULT { @add_lsr_imm %dest, %a, %b, %c; }
add a:IREG(64) sar b:IREG(64) c:IMM => dest:IRESULT { @add_asr_imm %dest, %a, %b, %c; }


sub a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { @subw %dest, %a, %b; }
sub a:IREG(8,16,32) b:IMM8 => dest:IRESULT { @subw_imm %dest, %a, %b; }
sub a:IREG(8,16,32) b:IMM8NEG => dest:IRESULT { @addw_imm %dest, %a, %b; }
sub a:IREG(8,16,32) shl b:IREG(8,16,32) c:IMM => dest:IRESULT { @subw_lsl_imm %dest, %a, %b, %c; }
sub a:IREG(8,16,32) shr b:IREG(8,16,32) c:IMM => dest:IRESULT { @subw_lsr_imm %dest, %a, %b, %c; }
sub a:IREG(8,16,32) sar b:IREG(8,16,32) c:IMM => dest:IRESULT { @subw_asr_imm %dest, %a, %b, %c; }

sub a:IREG(64) b:IREG(64) => dest:IRESULT { @sub %dest, %a, %b; }
sub a:IREG(64) b:IMM8 => dest:IRESULT { @sub_imm %dest, %a, %b; }
sub a:IREG(64) b:IMM8NEG => dest:IRESULT { @add_imm %dest, %a, %b; }
sub a:IREG(64) shl b:IREG(64) c:IMM => dest:IRESULT { @sub_lsl_imm %dest, %a, %b, %c; }
sub a:IREG(64) shr b:IREG(64) c:IMM => dest:IRESULT { @sub_lsr_imm %dest, %a, %b, %c; }
sub a:IREG(64) sar b:IREG(64) c:IMM => dest:IRESULT { @sub_asr_imm %dest, %a, %b, %c; }


#define MUL_INSTR(op, inst) \
op a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { @mulw %dest, %a, %b; } \
op a:IREG(64) b:IREG(64) => dest:IRESULT { @mul %dest, %a, %b; }
MUL_INSTR(smul, @smull)
MUL_INSTR(umul, @umull)

udiv a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { @udivw %dest, %a, %b; }
sdiv a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { @sdivw %dest, %a, %b; }

udiv a:IREG(64) b:IREG(64) => dest:IRESULT { @udiv %dest, %a, %b; }
sdiv a:IREG(64) b:IREG(64) => dest:IRESULT { @sdiv %dest, %a, %b; }

#define MOD_INSTR(op, inst) \
op a:IREG(8,16,32) b:IREG(8,16,32) => dest:IRESULT { \
	inst##w %dest, %a, %b; \
	@msubw %dest, %dest, %b, %a; \
} \
op a:IREG(64) b:IREG(64) => dest:IRESULT { \
	inst %dest, %a, %b; \
	@msub %dest, %dest, %b, %a; \
}
MOD_INSTR(umod, @udiv)
MOD_INSTR(smod, @sdiv)

#define BITWISE_OP(op, instr) \
op a:IREG(8,16,32) b:IREG(8,16,32) => dest:IREG { instr##w %dest, %a, %b; } \
op a:IREG(8,16,32) b:LOGICALIMM => dest:IREG { instr##w_imm %dest, %a, %b; } \
op a:IREG(8,16,32) shl b:IREG(8,16,32) c:IMM => dest:IREG { instr##w_lsl_imm %dest, %a, %b, %c; } \
op a:IREG(8,16,32) shr b:IREG(8,16,32) c:IMM => dest:IREG { instr##w_lsr_imm %dest, %a, %b, %c; } \
op a:IREG(8,16,32) sar b:IREG(8,16,32) c:IMM => dest:IREG { instr##w_asr_imm %dest, %a, %b, %c; } \
op a:IREG(64) b:IREG => dest:IREG(64) { instr %dest, %a, %b; } \
op a:IREG(64) b:LOGICALIMM => dest:IREG { instr##_imm %dest, %a, %b; } \
op a:IREG(64)shl b:IREG(64) c:IMM => dest:IREG { instr##_lsl_imm %dest, %a, %b, %c; } \
op a:IREG(64)shr b:IREG(64) c:IMM => dest:IREG { instr##_lsr_imm %dest, %a, %b, %c; } \
op a:IREG(64)sar b:IREG(64) c:IMM => dest:IREG { instr##_asr_imm %dest, %a, %b, %c; }
BITWISE_OP(and, @and)
BITWISE_OP(or, @orr)
BITWISE_OP(xor, @eor)

shl a:IREG(8,16,32) b:IMM => dest:IRESULT { @lslw_imm %dest, %a, %b; }
shl a:IREG(8,16,32) b:IREG => dest:IRESULT { @lslw %dest, %a, %b; }
shr a:IREG(8,16,32) b:IMM => dest:IRESULT { @lsrw_imm %dest, %a, %b; }
shr a:IREG(8,16,32) b:IREG => dest:IRESULT { @lsrw %dest, %a, %b; }
sar a:IREG(8,16,32) b:IMM => dest:IRESULT { @asrw_imm %dest, %a, %b; }
sar a:IREG(8,16,32) b:IREG => dest:IRESULT { @asrw %dest, %a, %b; }

shl a:IREG(64) b:IMM => dest:IRESULT { @lsl_imm %dest, %a, %b; }
shl a:IREG(64) b:IREG => dest:IRESULT { @lsl %dest, %a, %b; }
shr a:IREG(64) b:IMM => dest:IRESULT { @lsr_imm %dest, %a, %b; }
shr a:IREG(64) b:IREG => dest:IRESULT { @lsr %dest, %a, %b; }
sar a:IREG(64) b:IMM => dest:IRESULT { @asr_imm %dest, %a, %b; }
sar a:IREG(64) b:IREG => dest:IRESULT { @asr %dest, %a, %b; }

neg a:IREG(8,16,32) => dest:IRESULT { @negw %dest, %a; }
neg a:IREG(64) => dest:IRESULT { @negx %dest, %a; }
not a:IREG(8,16,32) => dest:IRESULT { @mvnw %dest, %a; }
not a:IREG(64) => dest:IRESULT { @mvnx %dest, %a; }

sconvert src:IREG(8) => dest:IREG(16,32,64) { @sxtb %dest, %src; }
uconvert src:IREG(8) => dest:IREG(16,32,64) { @uxtb %dest, %src; }
sconvert src:IREG(16) => dest:IREG(32,64) { @sxth %dest, %src; }
uconvert src:IREG(16) => dest:IREG(32,64) { @uxth %dest, %src; }
sconvert src:IREG(32) => dest:IREG(64) { @sxtw %dest, %src; }
uconvert src:IREG(32) => dest:IREG(64) { @movw %dest, %src; }
sconvert src:IREG(16,32,64) => dest:IREG(8) { @sxtb %dest, %src; }
uconvert src:IREG(16,32,64) => dest:IREG(8) { @uxtb %dest, %src; }
sconvert src:IREG(32,64) => dest:IREG(16) { @sxth %dest, %src; }
uconvert src:IREG(32,64) => dest:IREG(16) { @uxth %dest, %src; }
sconvert src:IREG(64) => dest:IREG(32) { @sxtw %dest, %src; }
uconvert src:IREG(64) => dest:IREG(32) { @and_imm %dest, %src, 0xffffffff; }

byteswap src:IREG(16) => dest:IREG { @rev16 %dest, %src; }
byteswap src:IREG(32) => dest:IREG { @rev32 %dest, %src; }
byteswap src:IREG(64) => dest:IREG { @rev64 %dest, %src; }

function void UnconditionalJump(SymInstrBlock* out, TreeBlock* block)
{
	if ((!m_settings.pad) && (block->GetSource()->GetGlobalIndex() == (m_currentBlock->GetSource()->GetGlobalIndex() + 1)))
	{
		// The destination block is the one just after the current one, just fall through
		return;
	}
	@b m_func, block->GetSource();
}

iftrue src:IREG t:BLOCK f:BLOCK { @cmpx_imm %src, 0; @bne m_func, %t->GetSource(); UnconditionalJump(out, %f); }

ife a:IREG(8,16,32) b:IREG(8,16,32) t:BLOCK f:BLOCK { @cmpw %a, %b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(8,16,32) b:IMM8 t:BLOCK f:BLOCK { @cmpw_imm %a, %b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(8,16,32) b:IMM8NEG t:BLOCK f:BLOCK { @cmnw_imm %a, -%b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(8,16,32) shl b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_lsl_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(8,16,32) shr b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_lsr_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(8,16,32) sar b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_asr_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }

ife a:IREG(64) b:IREG(64) t:BLOCK f:BLOCK { @cmpx %a, %b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(64) b:IMM8 t:BLOCK f:BLOCK { @cmpx_imm %a, %b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(64) b:IMM8NEG t:BLOCK f:BLOCK { @cmnx_imm %a, -%b; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(64) shl b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_lsl_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(64) shr b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_lsr_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }
ife a:IREG(64) sar b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_asr_imm %a, %b, %c; @beq m_func, %t->GetSource(); UnconditionalJump(out, %f); }

#define COND_COMPARE(op, instr) \
op a:IREG(8,16,32) b:IREG(8,16,32) t:BLOCK f:BLOCK { @cmpw %a, %b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(8,16,32) b:IMM8 t:BLOCK f:BLOCK { @cmpw_imm %a, %b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(8,16,32) b:IMM8NEG t:BLOCK f:BLOCK { @cmnw_imm %a, -%b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(8,16,32) shl b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_lsl_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(8,16,32) shr b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_lsr_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(8,16,32) sar b:IREG(8,16,32) c:IMM t:BLOCK f:BLOCK { @cmpw_asr_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) b:IREG(64) t:BLOCK f:BLOCK { @cmpx %a, %b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) b:IMM8 t:BLOCK f:BLOCK { @cmpx_imm %a, %b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) b:IMM8NEG t:BLOCK f:BLOCK { @cmnx_imm %a, -%b; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) shl b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_lsl_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) shr b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_lsr_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); } \
op a:IREG(64) sar b:IREG(64) c:IMM t:BLOCK f:BLOCK { @cmpx_asr_imm %a, %b, %c; instr m_func, %t->GetSource(); UnconditionalJump(out, %f); }
COND_COMPARE(ifslt, @blt)
COND_COMPARE(ifult, @bcc)
COND_COMPARE(ifsle, @ble)
COND_COMPARE(ifule, @bls)

goto dest:BLOCK { UnconditionalJump(out, %dest); }
goto dest:IREG { @br %dest; }

breakpoint { @brk; }

function void AdjustStackAfterCall(SymInstrBlock* out, uint32_t stackAdjust)
{
	if (stackAdjust != 0)
	{
		if (m_settings.stackGrowsUp)
			@sub_imm SYMREG_SP, SYMREG_SP, stackAdjust;
		else
			@add_imm SYMREG_SP, SYMREG_SP, stackAdjust;
	}
}

call func:FUNCTION reads:INPUT stackAdjust:IMM16 => dest:INTEGER_RETURN_VALUE
{
	@bl %func, %func->GetIL()[0], %dest, %reads;
	AdjustStackAfterCall(out, (uint32_t)%stackAdjust);
}

callvoid func:FUNCTION reads:INPUT stackAdjust:IMM16
{
	@bl %func, %func->GetIL()[0], SYMREG_NONE, %reads;
	AdjustStackAfterCall(out, (uint32_t)%stackAdjust);
}

call func:IREG reads:INPUT stackAdjust:IMM16 => dest:INTEGER_RETURN_VALUE
{
	@blr %func, %dest, %reads;
	AdjustStackAfterCall(out, (uint32_t)%stackAdjust);
}

callvoid func:IREG reads:INPUT stackAdjust:IMM16
{
	@blr %func, SYMREG_NONE, %reads;
	AdjustStackAfterCall(out, (uint32_t)%stackAdjust);
}

function bool GenerateReturnVoid(SymInstrBlock* out)
{
	// Restore frame pointer (if present) and adjust stack
	if (m_framePointerEnabled)
	{
		@mov SYMREG_SP, SYMREG_BP;
	}
	else
	{
		// TODO: Support frames without a frame pointer
		return false;
	}

	// The restore register function also pops PC off the stack
	@restoreregs;
	@ret SYMREG_LR;
	return true;
}

return src:IREG, retval:INTEGER_RETURN_VALUE { @mov %retval, %src; GenerateReturnVoid(out); @symreturn %retval; }
returnvoid { GenerateReturnVoid(out); }

alloca size:IREG => result:IREG, temp:IREG
{
	if (m_settings.stackGrowsUp)
	{
		@add_imm SYMREG_SP, SYMREG_SP, 16;
		@mov %result, SYMREG_SP;
		@add SYMREG_SP, SYMREG_SP, %size;
		@movn %temp, 15;
		@and SYMREG_SP, SYMREG_SP, %temp;
	}
	else
	{
		@mov %temp, SYMREG_SP;
		@sub %temp, %temp, %size;
		@and_imm SYMREG_SP, %temp, 0xfffffffffffffff0;
		@mov %result, SYMREG_SP;
	}
}

alloca size:IMM8 => result:IREG, temp:IREG
{
	if (m_settings.stackGrowsUp)
	{
		@add_imm SYMREG_SP, SYMREG_SP, 16;
		@mov %result, SYMREG_SP;
		@add_imm SYMREG_SP, SYMREG_SP, %size;
		@movn %temp, 15;
		@and SYMREG_SP, SYMREG_SP, %temp;
	}
	else
	{
		@sub_imm SYMREG_SP, SYMREG_SP, %size;
		@and_imm SYMREG_SP, %temp, 0xfffffffffffffff0;
		@mov %result, SYMREG_SP;
	}
}

vararg => result:IREG, temp:IREG { @add_stack %result, SYMREG_BP, m_varargStart, 0, %temp; }

push offset src:IREG arg:IMM argc:IMM
{
	int64_t argSize = ((argc * 8) + 15) & ~0xf;
	int64_t argOffset = arg * 8;

	if (arg != 0)
		@strx_imm %src, SYMREG_SP, -argSize + argOffset;
	else
		@strx_imm_pre %src, SYMREG_SP, -argSize + argOffset;
}

syscall num:IMM16 reads:INPUT stackAdjust:IMM => dest:SYSCALL_RETURN, numreg:SYSCALL_NUM
{
	vector<uint32_t> writes;
	writes.push_back(%dest);
	@movz %numreg, %num;
	@svc %numreg, writes, %reads;
}

syscall num:IREG reads:INPUT stackAdjust:IMM => dest:SYSCALL_RETURN, numreg:SYSCALL_NUM
{
	vector<uint32_t> writes;
	writes.push_back(%dest);
	@mov %numreg, %num;
	@svc %numreg, writes, %reads;
}

peb => dest:IREG  { @mov %dest, 18; }

function void GenerateAntiDisassembly(SymInstrBlock* out)
{
}

function bool GenerateFunctionStart(SymInstrBlock* out)
{
	if ((m_func->GetName() == "_start") && m_settings.unsafeStack)
	{
		// This is the start function, and we can't assume we have a safe stack (the code may be
		// at or near the stack pointer), pivot the stack to make it safe
		uint32_t reg = TEMP_REGISTER(IREG);
		uint32_t reg2 = TEMP_REGISTER(IREG);
		@mov reg, SYMREG_SP;
		@movz reg2, UNSAFE_STACK_PIVOT;
		@sub reg, reg, reg2;
		@and_imm SYMREG_SP, reg, 0xfffffffffffffff0;
	}

	// Generate function prologue
	@saveregs;
	if (m_framePointerEnabled)
	{
		@mov SYMREG_BP, SYMREG_SP;
	}

	// Generate a pseudo instruction to ensure the incoming parameters are defined
	vector<uint32_t> incomingRegs;
	for (vector<IncomingParameterCopy>::iterator j = m_paramCopy.begin(); j != m_paramCopy.end(); j++)
	{
		if (j->stackVar != SYMREG_NONE)
			continue;

		incomingRegs.push_back(j->incomingReg);
	}

	if (incomingRegs.size() != 0)
		@regparam incomingRegs;

	// Copy parameters into variables so that they can be spilled if needed
	for (vector<IncomingParameterCopy>::iterator j = m_paramCopy.begin(); j != m_paramCopy.end(); j++)
	{
		if (j->stackVar != SYMREG_NONE)
		{
			// Parameter was spilled onto stack
			switch (j->var->GetType()->GetWidth())
			{
			case 1:
				@strb_stack j->incomingReg, SYMREG_BP, j->stackVar, 0, TEMP_REGISTER(IREG);
				break;
			case 2:
				@strh_stack j->incomingReg, SYMREG_BP, j->stackVar, 0, TEMP_REGISTER(IREG);
				break;
			case 4:
				@strw_stack j->incomingReg, SYMREG_BP, j->stackVar, 0, TEMP_REGISTER(IREG);
				break;
			case 8:
				@strx_stack j->incomingReg, SYMREG_BP, j->stackVar, 0, TEMP_REGISTER(IREG);
				break;
			default:
				fprintf(stderr, "error: spilling invalid parameter\n");
				return false;
			}
		}
		else
		{
			// Parameter is in an integer register
			uint32_t newReg = TEMP_REGISTER(IREG);
			@mov newReg, j->incomingReg;
			m_vars.registerVariables[j->var] = newReg;
		}
	}

	if (m_framePointerEnabled)
	{
		uint32_t temp = TEMP_REGISTER(IREG);
		if (m_settings.stackGrowsUp)
			@add_stack SYMREG_SP, SYMREG_SP, SYMVAR_FRAME_SIZE, 0, temp;
		else
			@sub_stack SYMREG_SP, SYMREG_SP, SYMVAR_FRAME_SIZE, 0, temp;
	}

	return true;
}

arch function set<uint32_t> GetRegisterClassInterferences(uint32_t cls)
{
	set<uint32_t> result;
	return result;
}

arch function bool DoesRegisterClassConflictWithSpecialRegisters(uint32_t cls)
{
	return false;
}

arch function void LayoutStackFrame()
{
	// Lay out stack variables
	int64_t offset = 0;
	for (size_t i = 0; i < m_stackVarOffsets.size(); i++)
	{
		if (m_stackVarIsParam[i])
			continue;

		int64_t align = 1;
		if (m_stackVarWidths[i] >= 8)
			align = 8;
		else if (m_stackVarWidths[i] >= 4)
			align = 4;
		else if (m_stackVarWidths[i] >= 2)
			align = 2;

		if ((offset & (align - 1)) != 0)
			offset += align - (offset & (align - 1));

		m_stackVarOffsets[i] = offset;
		offset += m_stackVarWidths[i];
	}

	// Ensure stack stays aligned on native boundary
	if (offset & 0xf)
		offset += 0x10 - (offset & 0xf);

	m_stackFrameSize = offset;

	// Adjust variable offsets to be relative to the frame pointer (negative offsets)
	if (m_settings.stackGrowsUp)
	{
		for (size_t i = 0; i < m_stackVarOffsets.size(); i++)
		{
			if (!m_stackVarIsParam[i])
				m_stackVarOffsets[i] -= offset;
		}
	}

	for (size_t i = 0; i < m_stackVarOffsets.size(); i++)
	{
		if (m_stackVarIsParam[i])
			continue;
		if (m_settings.stackGrowsUp)
			m_stackVarOffsets[i] += m_stackFrameSize;
		else
			m_stackVarOffsets[i] -= m_stackFrameSize;
	}

	// Account for callee saved registers
	int32_t adjust = 16 + (m_clobberedCalleeSavedRegs.size() * 8);
	adjust = (adjust + 0xf) & ~0xf;
	for (size_t i = 0; i < m_stackVarOffsets.size(); i++)
	{
		if (!m_stackVarIsParam[i])
			continue;

		if (m_settings.stackGrowsUp)
			m_stackVarOffsets[i] -= adjust;
		else
			m_stackVarOffsets[i] += adjust;
	}
}

arch function bool GenerateSpillLoad(uint32_t reg, uint32_t var, int64_t offset,
	ILParameterType type, vector<SymInstr*>& code)
{
	uint32_t temp = AddRegister(IREG);
	switch (type)
	{
	case ILTYPE_INT8:
		@ldrb_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT16:
		@ldrh_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT32:
		@ldrw_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT64:
		@ldrx_stack reg, SYMREG_BP, var, offset, temp;
		break;
	default:
		return false;
	}

	return true;
}

arch function bool GenerateSpillStore(uint32_t reg, uint32_t var, int64_t offset,
	ILParameterType type, vector<SymInstr*>& code)
{
	uint32_t temp = AddRegister(IREG);
	switch (type)
	{
	case ILTYPE_INT8:
		@strb_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT16:
		@strh_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT32:
		@strw_stack reg, SYMREG_BP, var, offset, temp;
		break;
	case ILTYPE_INT64:
		@strx_stack reg, SYMREG_BP, var, offset, temp;
		break;
	default:
		return false;
	}

	return true;
}

arch function void PrintRegister(uint32_t reg)
{
	if (reg == SYMREG_NATIVE_REG(REG_FP))
		fprintf(stderr, "fp");
	else if (reg == SYMREG_NATIVE_REG(REG_LR))
		fprintf(stderr, "lr");
	else if (reg == SYMREG_NATIVE_REG(REG_SP))
		fprintf(stderr, "sp");
	else if ((reg >= SYMREG_NATIVE_REG(0)) && (reg <= SYMREG_NATIVE_REG(30)))
		fprintf(stderr, "x%d", reg - SYMREG_NATIVE_REG(0));
	else
		SymInstrFunction::PrintRegister(reg);
}

